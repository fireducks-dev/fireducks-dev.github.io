<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>FireDucks â€“ Docs</title><link>https://fireducks-dev.github.io/docs/</link><description>Recent content in Docs on FireDucks</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 06 Oct 2020 08:48:23 +0000</lastBuildDate><atom:link href="https://fireducks-dev.github.io/docs/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Introduction</title><link>https://fireducks-dev.github.io/docs/user-guide/01-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://fireducks-dev.github.io/docs/user-guide/01-intro/</guid><description>
&lt;p>FireDucks has the same API as in pandas, so you can refer to the pandas documentation and articles to get started, or you can use LLM to output pandas code and run it in FireDucks.&lt;/p>
&lt;p>This user guide is intended for those who want to learn more about FireDucks, explaining its inner workings and what is useful to know about it. In particular, the following points may seem strange to those who are familiar with pandas.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://fireducks-dev.github.io/docs/user-guide/02-exec-model/">Execution model&lt;/a>
&lt;ul>
&lt;li>For those who are used to pandas, the difference between the execution model of pandas and FireDucks may be a concern. In that case, please read &lt;a href="https://fireducks-dev.github.io/docs/user-guide/02-exec-model/">Execution Model&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://fireducks-dev.github.io/docs/user-guide/04-compatibility/">pandas compatibility&lt;/a>
&lt;ul>
&lt;li>This section describes pandas compatibility from the perspective of compatibility, which is not the goal of FireDucks.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://fireducks-dev.github.io/docs/user-guide/05-api/#pandas-conversion">Using external libraries&lt;/a>
&lt;ul>
&lt;li>Due to the incompatibility that FireDucks does not aim for, FireDucks data may not work when passed to a library that accepts pandas data. In such cases, the data can be converted to pandas data.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: Execution Model</title><link>https://fireducks-dev.github.io/docs/user-guide/02-exec-model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://fireducks-dev.github.io/docs/user-guide/02-exec-model/</guid><description>
&lt;p>The execution model of FireDucks differs from that of pandas. pandas is an eager execution model in which the process is executed immediately upon method invocation, while FireDucks is a lazy execution model in which the process is executed in batches when the results are needed.&lt;/p>
&lt;h2 id="lazy-execution-model">Lazy execution model&lt;/h2>
&lt;p>The following figure shows the execution images of pandas and FireDucks.&lt;/p>
&lt;p>&lt;img src="../exec_model.webp" alt="execution model">&lt;/p>
&lt;p>In pandas, for example, calling the &lt;code>read_csv&lt;/code> method reads data from a CSV file. FireDucks, on the other hand, only generates an intermediate language equivalent to &lt;code>read_csv&lt;/code>, but does not read the data. Therefore, the line &lt;code>df = pd.read_csv(&amp;quot;data.csv&amp;quot;)&lt;/code> ends immediately in FireDucks.&lt;/p>
&lt;p>Thus, the main methods of FireDucks do not actually process the data frame, but generate the intermediate language. Each time a method is called, more and more intermediate language is generated, and when the result is needed (e.g., when writing to a csv file), the previously generated intermediate language is executed all at once.&lt;/p>
&lt;p>FireDucks performs actual execution at the following points&lt;/p>
&lt;ul>
&lt;li>Saving to a file (&lt;code>DataFrame.to_csv&lt;/code>, &lt;code>DataFrame.to_parquet&lt;/code>, etc.)&lt;/li>
&lt;li>Displaying a data frame (&lt;code>print(df)&lt;/code>, etc.)&lt;/li>
&lt;/ul>
&lt;p>Because of these differences in execution models, those familiar with pandas may find that time-consuming methods finish in a fraction of a second, or that saving to a csv file takes longer than expected.&lt;/p>
&lt;h2 id="about-time-measurement">About time measurement&lt;/h2>
&lt;p>FireDucks performs delayed execution, so if you want to measure the actual processing time for each method, a little additional effort is required.&lt;/p>
&lt;p>For example, if you insert timers are below, the last interval (&lt;code>t3 - t2&lt;/code>) will include the time of all processes.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>t0 &lt;span style="color:#f92672">=&lt;/span> time&lt;span style="color:#f92672">.&lt;/span>time()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>df &lt;span style="color:#f92672">=&lt;/span> pd&lt;span style="color:#f92672">.&lt;/span>read_csv(&lt;span style="color:#e6db74">&amp;#34;data.csv&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>t1 &lt;span style="color:#f92672">=&lt;/span> time&lt;span style="color:#f92672">.&lt;/span>time()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>df &lt;span style="color:#f92672">=&lt;/span> df&lt;span style="color:#f92672">.&lt;/span>sort_values(&lt;span style="color:#e6db74">&amp;#34;a&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>t2 &lt;span style="color:#f92672">=&lt;/span> time&lt;span style="color:#f92672">.&lt;/span>time()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>df&lt;span style="color:#f92672">.&lt;/span>to_csv(&lt;span style="color:#e6db74">&amp;#34;sorted.csv&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>t3 &lt;span style="color:#f92672">=&lt;/span> time&lt;span style="color:#f92672">.&lt;/span>time()
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>From version 0.9.1, &lt;strong>benchmark mode&lt;/strong> has been introduced for this purpose.
When bechmark mode is enabled, FireDucks actually executes a method immediately
after it is called. Since this disables some optimizations of FireDucks, it
should be used only when you want to measure individual methods.&lt;/p>
&lt;p>To enable benchmark mode, please use environment variable:&lt;/p>
&lt;pre tabindex="0">&lt;code>FIREDUCKS_FLAGS=&amp;#34;--benchmark-mode&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Or it can be enabled in a code as:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> fireducks.core &lt;span style="color:#f92672">import&lt;/span> get_fireducks_options
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>get_fireducks_options()&lt;span style="color:#f92672">.&lt;/span>set_benchmark_mode(&lt;span style="color:#66d9ef">True&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The other workaround is calling FireDucks&amp;rsquo; own method &lt;a href="https://fireducks-dev.github.io/docs/user-guide/05-api/#_evaluate">&lt;code>_evaluate&lt;/code>&lt;/a> for immediate execution.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>t0 &lt;span style="color:#f92672">=&lt;/span> time&lt;span style="color:#f92672">.&lt;/span>time()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>df &lt;span style="color:#f92672">=&lt;/span> pd&lt;span style="color:#f92672">.&lt;/span>read_csv(&lt;span style="color:#e6db74">&amp;#34;data.csv&amp;#34;&lt;/span>)&lt;span style="color:#f92672">.&lt;/span>_evaluate()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>t1 &lt;span style="color:#f92672">=&lt;/span> time&lt;span style="color:#f92672">.&lt;/span>time()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>df &lt;span style="color:#f92672">=&lt;/span> df&lt;span style="color:#f92672">.&lt;/span>sort_values(&lt;span style="color:#e6db74">&amp;#34;a&amp;#34;&lt;/span>)&lt;span style="color:#f92672">.&lt;/span>_evaluate()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>t2 &lt;span style="color:#f92672">=&lt;/span> time&lt;span style="color:#f92672">.&lt;/span>time()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>df&lt;span style="color:#f92672">.&lt;/span>to_csv(&lt;span style="color:#e6db74">&amp;#34;sorted.csv&amp;#34;&lt;/span>)&lt;span style="color:#f92672">.&lt;/span>_evaluate()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>t3 &lt;span style="color:#f92672">=&lt;/span> time&lt;span style="color:#f92672">.&lt;/span>time()
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="point-to-be-noted">Point to be noted&lt;/h2>
&lt;p>It is advised to refrain from using the conventional way of checking &lt;code>KeyError&lt;/code> using &lt;strong>try-catch block&lt;/strong>, when using
FireDucks default mode of lazy-execution. For example, consider the below example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">project&lt;/span>(df, cname):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">try&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> s &lt;span style="color:#f92672">=&lt;/span> df[cname]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">except&lt;/span> &lt;span style="color:#a6e22e">KeyError&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(&lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Column &lt;/span>&lt;span style="color:#e6db74">{&lt;/span>cname&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74"> doesn&amp;#39;t exist in input dataframe&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Due to lazy-execution, &lt;code>s = df[cname]&lt;/code> will not actually be executed right after it is called.
Hence the try-catch execution might not work as expected in this case. If you cannot avoid the above
code structure due to some restriction, it is advised to call it as &lt;code>s = df[cname]._evaluate()&lt;/code> to
enforce the statement to be executed at that very point.&lt;/p>
&lt;p>Otherwise, it is advised to use more standard way to implement the above case as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">project&lt;/span>(df, cname):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> cname &lt;span style="color:#f92672">not&lt;/span> &lt;span style="color:#f92672">in&lt;/span> df:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(&lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Column &lt;/span>&lt;span style="color:#e6db74">{&lt;/span>cname&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74"> doesn&amp;#39;t exist in input dataframe&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> s &lt;span style="color:#f92672">=&lt;/span> df[cname]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>in&lt;/code> operator invokes DataFrame.__contains__() which is a non-lazy method,
since it returns the result to be either True or False and needs to be evaluated right
after it is called.&lt;/p></description></item><item><title>Docs: Acceleration in FireDucks</title><link>https://fireducks-dev.github.io/docs/user-guide/03-acceleration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://fireducks-dev.github.io/docs/user-guide/03-acceleration/</guid><description>
&lt;p>There are two mechanisms to accelerate FireDucks. The first is &lt;a href="#compiler-accelerated">compiler optimization&lt;/a> on the IR, intermediate representation, and the second is &lt;a href="#multithreading">multithreading&lt;/a> on the backend.&lt;/p>
&lt;h3 id="compiler-accelerated">Compiler Optimization&lt;/h3>
&lt;p>FireDucks uses a runtime compiler mechanism to convert Python programs into an intermediate language before execution. Optimization on the intermediate language means that the Python program is converted to an intermediate language that can be executed faster without changing the meaning of the program, rather than being executed as-is. This is equivalent to automatically performing the kind of tuning that a skilled programmer would perform when writing a program.&lt;/p>
&lt;p>The FireDucks intermediate language is an intermediate language designed specifically for DataFrames, and each instruction in the intermediate language is a highly abstract, information-rich instruction that represents an operation on a DataFrame. Therefore, the FireDucks compiler can understand the meaning of a program without complicated program analysis and can perform optimization specific to DataFrame.&lt;/p>
&lt;h4 id="examples-of-optimizations">Examples of Optimizations&lt;/h4>
&lt;p>Here is an example of such optimization. The following program extracts rows from a DataFrame in which column a is greater than 10, and then extracts column b from that row.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>selected &lt;span style="color:#f92672">=&lt;/span> df[df[&lt;span style="color:#e6db74">&amp;#34;a&amp;#34;&lt;/span>] &lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#ae81ff">10&lt;/span>][&lt;span style="color:#e6db74">&amp;#34;b&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This is a commonly used process, and the code looks straightforward. However, the process of extracting the first row covers all columns, which is not efficient if df has more columns than just a and b. Because DataFrames generally use column-oriented data structures, the process of extracting a specific row is a much more time-consuming process than extracting a column, and doing so for all columns may result in non-negligible overhead.&lt;/p>
&lt;p>FireDucks optimization uses such domain knowledge to transform the intermediate language so that column extraction is performed first. The transformed process, written in Python, looks like the following code.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>tmp &lt;span style="color:#f92672">=&lt;/span> df[[&lt;span style="color:#e6db74">&amp;#34;a&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;b&amp;#34;&lt;/span>]]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>selected &lt;span style="color:#f92672">=&lt;/span> tmp[df[&lt;span style="color:#e6db74">&amp;#34;a&amp;#34;&lt;/span>] &lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#ae81ff">10&lt;/span>][&lt;span style="color:#e6db74">&amp;#34;b&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>While skilled users who are aware of DataFrame&amp;rsquo;s internal data structures will prefer code like this, FireDucks performs this acceleration as an optimization on an intermediate language.&lt;/p>
&lt;h3 id="multithreading">Multithreading&lt;/h3>
&lt;p>In FireDucks, the user API and its execution are completely independent of each other via an intermediate language. The backend executes the instructions of the intermediate language and has the ability to perform specific data structures and operations on data frames.&lt;/p>
&lt;p>FireDucks allows the backend to be changed, for example, a backend tuned for multi-core, a backend using accelerators such as GPUs, etc., to match the target environment, thereby increasing speed. The backend can be changed by environment variables, allowing the user to switch backends without changing the user program at all.&lt;/p>
&lt;p>FireDucks includes a multi-threaded backend for CPUs. This backend uses Apache Arrow as the data structure and adds its own parallelization in addition to the data frame operations provided by Apache Arrow.&lt;/p></description></item><item><title>Docs: pandas compatibility</title><link>https://fireducks-dev.github.io/docs/user-guide/04-compatibility/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://fireducks-dev.github.io/docs/user-guide/04-compatibility/</guid><description>
&lt;p>FireDucks provides the same API (class names, method names, and attribute names) as pandas, and aims for compatibility in terms of being able to use it simply by changing import statements.&lt;/p>
&lt;h2 id="compatibility-concept">Compatibility Concept&lt;/h2>
&lt;p>We do &lt;em>not&lt;/em> aim for compatibility in the following aspects.&lt;/p>
&lt;ul>
&lt;li>Complete consistency of class names
&lt;ul>
&lt;li>FireDucks provides a pandas-compatible API in the module &lt;code>fireducks.pandas&lt;/code>. The complete class names, including module names, are different from those of pandas.&lt;/li>
&lt;li>For example, the data frame type is &lt;code>pandas.DataFrame&lt;/code> in pandas, but &lt;code>fireducks.pandas.DataFrame&lt;/code> in FireDucks, which are not exactly the same. Therefore, an explicit test for a pandas DataFrame, such as &lt;code>isinstance(df, pandas.DataFrame)&lt;/code>, will be false. True for &lt;code>import fireducks.pandas as pd; isinstance(df, pd.DataFrame)&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Full compatibility of errors and warnings
&lt;ul>
&lt;li>Because of delayed execution, the timing of errors and warnings in FireDucks is different from that in pandas.&lt;/li>
&lt;li>It is not the goal to match error messages (although the goal is to match Exception classes).&lt;/li>
&lt;li>In addition, since Warnings may not be necessary in FireDucks, whether or not a Warning is generated may not match pandas.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Complete reproduction of undefined behavior and bugs in pandas.
&lt;ul>
&lt;li>We do not aim to reproduce the implementation-dependent behavior of pandas.&lt;/li>
&lt;li>In particular, it may be undefined whether pandas returns a copy or a reference, and we do not recommend writing code that depends on either (&lt;a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#why-does-assignment-fail-when-using-chained-indexing">reference&lt;/a>).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>pandas internal API and Experimental features
&lt;ul>
&lt;li>We do not aim to provide methods that begin with an underscore (&lt;code>_&lt;/code>) or features that are marked as Experimental in the pandas documentation.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Extending pandas
&lt;ul>
&lt;li>Functions that extend pandas as described in &lt;a href="https://pandas.pydata.org/docs/development/extending.html">Extending pandas&lt;/a> are currently not targeted.&lt;/li>
&lt;li>These include the ability to create subclasses of DataFrames and Series, and the ability to define your own data types.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Consistency of the order of merge/join result rows with pandas
&lt;ul>
&lt;li>The order of rows in merge/join results may not match the order in pandas. Sorting will match.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>copy(deep=False) might not work when changes made in data values of &amp;lsquo;copied&amp;rsquo; instance is expected to be reflected in the data values of the
&amp;lsquo;source&amp;rsquo; instance. The changes made in the metadata will work as expected. &lt;a href="https://fireducks-dev.github.io/docs/user-guide/04-compatibility/#copydeep--false">Refer&lt;/a> for more details.&lt;/li>
&lt;/ul>
&lt;h2 id="copydeep--false">copy(deep = False)&lt;/h2>
&lt;p>In pandas, df.copy(deep=False) is mostly used for the following two purposes:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>when we just want to modifiy the metadata of a table (like column name etc.), but not the actual content as follows:
&lt;img src="../copy1.png" alt="change_metadata">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>when we want the modification made in the &amp;lsquo;copied&amp;rsquo; (df2) instance to be reflected in the &amp;lsquo;source&amp;rsquo; (df1) instance as follows. This is more likely a side-effect of the shallow-copy, instead of what a user might want to perform.
&lt;img src="../copy2.png" alt="change_data_values">&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>FireDucks actually doesn&amp;rsquo;t support in-place operations &lt;em>in true sense&lt;/em>,
since it assumes all the data instances are immutable in nature (to improve performance).
Whenever an in-place operation on a column is performed, e.g. &lt;code>df[&amp;quot;A&amp;quot;] += 2&lt;/code>,
it allocates a new memory for that column to write the result of the operation and
modifies the data pointer as shown below to make it look like an in-place operation.
&lt;img src="../in_place_op.jpg" alt="in_place_addition">&lt;/p>
&lt;p>Hence, (1) will work as expected in FireDucks (because it doesn&amp;rsquo;t change the actual data values),
but (2) will not work as expected, since the change on copied instance is reflected on a newly allocated memory
which is pointed by the &amp;lsquo;copied&amp;rsquo; instance (df2),
but the &amp;lsquo;source&amp;rsquo; instance (df1) still points to the initial memory as depicted in following figure:
&lt;img src="../shallow_copy_issue.jpg" alt="shallow_copy_issue">.&lt;/p>
&lt;p>Usage similar to (1) can be found in some internal implementation of libraries like seaborn, skimpy etc.
Hence you can &lt;strong>safely ignore the warning message&lt;/strong>, when using these libraries with import-hook enabled in FireDucks.&lt;/p>
&lt;h2 id="use-with-pandas">Use with pandas&lt;/h2>
&lt;p>The internal API and &lt;code>isinstance&lt;/code> are frequently used in pandas. Therefore, &lt;strong>using FireDucks with pandas will not work in most cases.&lt;/strong> It is recommended to rewrite all import statements or use &lt;a href="https://fireducks-dev.github.io/docs/get-started/#import-hook">automatic conversion by import hook&lt;/a> to use FireDucks instead.&lt;/p>
&lt;p>If for some reason you wish to use FireDucks with pandas or pass DataFrame or Series of FireDucks to a library that accepts those of pandas, please use the &lt;a href="https://fireducks-dev.github.io/docs/user-guide/05-api/#pandas-conversion">pandas conversion function&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> somelib
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> fireducks.pandas &lt;span style="color:#66d9ef">as&lt;/span> pd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>df &lt;span style="color:#f92672">=&lt;/span> pd&lt;span style="color:#f92672">.&lt;/span>read_csv(&lt;span style="color:#f92672">...&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>somelib&lt;span style="color:#f92672">.&lt;/span>process_pandas_dataframe(df&lt;span style="color:#f92672">.&lt;/span>to_pandas())
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: FireDucks Own API</title><link>https://fireducks-dev.github.io/docs/user-guide/05-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://fireducks-dev.github.io/docs/user-guide/05-api/</guid><description>
&lt;p>FireDucks has its own API that some pandas do not have. Here are some of them.&lt;/p>
&lt;h3 id="pandas-conversion">pandas conversion&lt;/h3>
&lt;p>FireDucks DataFrame/Series has a &lt;code>to_pandas&lt;/code> method that allows conversion to pandas data. This is useful, for example, when using an external library that accepts pandas data.&lt;/p>
&lt;p>Also, &lt;code>fireducks.pandas.from_pandas&lt;/code> can be used to convert pandas DataFrames/Series to FireDucks.&lt;/p>
&lt;h3 id="_evaluate">Explicit intermediate language execution&lt;/h3>
&lt;p>FireDucks provides lazy execution. Lazy execution allows multiple APIs to be executed at once and is an important feature for speeding up the process through optimization on intermediate languages.&lt;/p>
&lt;p>On the other hand, when you want to measure the execution time of individual APIs, such as when verifying the operation of FireDucks, you will need to be creative; most APIs in FireDucks only generate an intermediate language, so they are completed in a very short time, and measuring before and after API calls does not allow you to measure the actual data frame processing time. Therefore, it is not possible to measure the actual data frame processing time by measuring the time before and after the API call.&lt;/p>
&lt;p>In such cases, the &lt;code>DataFrame._evaluate&lt;/code> method can be used to explicitly execute the API. When &lt;code>_evaluate&lt;/code> is called, it executes the intermediate language for the DataFrame that has been created up to that point in time. Therefore, by calling &lt;code>_evaluate&lt;/code> before and after the process you want to measure, you can measure the actual processing time.&lt;/p>
&lt;p>An example of groupby time measurement is shown below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>df &lt;span style="color:#f92672">=&lt;/span> pd&lt;span style="color:#f92672">.&lt;/span>read_csv(&lt;span style="color:#f92672">...&lt;/span>)&lt;span style="color:#f92672">.&lt;/span>_evaluate() &lt;span style="color:#75715e"># end read_csv so that it is not in the measurement range&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>t0 &lt;span style="color:#f92672">=&lt;/span> time&lt;span style="color:#f92672">.&lt;/span>time() &lt;span style="color:#75715e"># start measuring time&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>g &lt;span style="color:#f92672">=&lt;/span> df&lt;span style="color:#f92672">.&lt;/span>groupby(&lt;span style="color:#f92672">...&lt;/span>)&lt;span style="color:#f92672">.&lt;/span>sum()&lt;span style="color:#f92672">.&lt;/span>_evaluate() &lt;span style="color:#75715e"># immediately after generating intermediate language for groupby&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>t1 &lt;span style="color:#f92672">=&lt;/span> time&lt;span style="color:#f92672">.&lt;/span>time() &lt;span style="color:#75715e"># end of time measurement&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="api-for-feature-generation">API for feature generation&lt;/h3>
&lt;p>One of the applications of data frames is feature generation, which is a pre-processing step in machine learning. In feature generation, data frames are processed in various ways to generate features for training in order to create better models, which can be very time-consuming.&lt;/p>
&lt;p>FireDucks also provides an API for fast feature generation. Currently, the following two features are typical feature generation methods. These features can be implemented using a combination of pandas APIs, but FireDucks provides them as APIs, and they are preoptimized to be fast, just as the FireDucks compiler does. See the API Doc for details on each API.&lt;/p>
&lt;ul>
&lt;li>Aggregate features: &lt;code>fireducks.pandas.aggregate&lt;/code>&lt;/li>
&lt;li>multi-target encoding: &lt;code>fireducks.pandas.multi_target_encoding&lt;/code>&lt;/li>
&lt;/ul></description></item><item><title>Docs: Tips</title><link>https://fireducks-dev.github.io/docs/user-guide/tips/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://fireducks-dev.github.io/docs/user-guide/tips/</guid><description>
&lt;p>The know-how of pandas, such as avoiding loops and apply, is also useful for FireDucks. Here are some tips to improve performance in FireDucks.&lt;/p>
&lt;h2 id="avoid-loops">Avoid loops&lt;/h2>
&lt;p>Looping out data from a DataFrame is slow, so it is better to use the DataFrame API as much as possible (this is also true for pandas).&lt;/p>
&lt;p>For example, the following loop processes the elements of a Series one by one.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>s &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> i &lt;span style="color:#f92672">in&lt;/span> range(len(df)):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> df[&lt;span style="color:#e6db74">&amp;#34;A&amp;#34;&lt;/span>][i] &lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> s &lt;span style="color:#f92672">+=&lt;/span> df[&lt;span style="color:#e6db74">&amp;#34;B&amp;#34;&lt;/span>][i]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Using the API, you can write the following.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>s &lt;span style="color:#f92672">=&lt;/span> df[df[&lt;span style="color:#e6db74">&amp;#34;A&amp;#34;&lt;/span>] &lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>][&lt;span style="color:#e6db74">&amp;#34;B&amp;#34;&lt;/span>]&lt;span style="color:#f92672">.&lt;/span>sum()
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The same applies to &lt;code>Dataframe.iterrows&lt;/code> and so on.&lt;/p>
&lt;h2 id="do-not-use-apply">Do not use apply&lt;/h2>
&lt;p>Passing user-defined functions such as &lt;code>DataFrame.apply&lt;/code> is not currently supported by FireDucks&amp;rsquo; current optimizer that generates an intermediate language and compiles it.&lt;/p>
&lt;h2 id="do-not-use-attribute-style-column-references">Do not use attribute-style column references&lt;/h2>
&lt;p>Column references can be written as &lt;code>df[&amp;quot;A&amp;quot;]&lt;/code> or &lt;code>df.A&lt;/code>, but the latter may conflict with the original attributes of the DataFrame, so it is better to write &lt;code>df[&amp;quot;A&amp;quot;]&lt;/code> in bracket format.&lt;/p>
&lt;p>In FireDucks, the &lt;code>df.A&lt;/code> format requires processing to determine whether &lt;code>A&lt;/code> is a column name or not, which may result in loss of compiler optimization.&lt;/p>
&lt;h2 id="do-not-use-undefined-behavior-of-pandas">Do not use undefined behavior of pandas&lt;/h2>
&lt;p>Whether &lt;code>df[&amp;quot;A&amp;quot;]&lt;/code> returns a reference or a copy in the following cases is undefined in pandas, but if it returns a copy, &lt;code>df&lt;/code> is not updated. In many cases this will not be the intended behavior.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>df[&lt;span style="color:#e6db74">&amp;#34;A&amp;#34;&lt;/span>][&lt;span style="color:#ae81ff">1&lt;/span>] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Such undefined behavior may work differently in FireDucks than in pandas, so what works in pandas may not work in FireDucks. It is better not to use undefined behavior.&lt;/p>
&lt;p>In this example, it is safe to write the following. However, as mentioned above, element-by-element access is inefficient, so if another implementation is possible, it is better to use it.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>df&lt;span style="color:#f92672">.&lt;/span>iloc[&lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">0&lt;/span>] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#75715e"># if A is the first column&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="avoid-fallback">Avoid Fallback&lt;/h2>
&lt;p>FireDucks has a feature called fallback that calls pandas internally. This is a feature to improve pandas compatibility by using pandas to perform functions not currently supported by FireDucks. On the other hand, this is a disadvantage in terms of execution time and memory usage, since it involves converting the FireDucks data structure to pandas, executing the pandas method, and then converting it to the FireDucks data structure again.&lt;/p>
&lt;p>We are continuously working to reduce fallbacks, but it is also effective to improve performance by avoiding fallbacks in user programs. The environment variable &lt;code>FIREDUCKS_FLAGS=&amp;quot;-Wfallback&amp;quot;&lt;/code> can be used to log when fallbacks occur.&lt;/p>
&lt;h2 id="profiling-kernel-wise-performance-when-using-notebook">Profiling kernel-wise performance when using notebook&lt;/h2>
&lt;p>From &lt;strong>FireDucks 0.10.9&lt;/strong>, we have provided some magic methods to profile the execution time of each frame/series related methods when running from Jupyter Notebook.
Use &lt;code>%load_ext fireducks.ipyext&lt;/code> to load the ipython extension module and then apply the cell-magic as &lt;code>%%fireducks.profile&lt;/code> in the target cell you want to trace.&lt;/p>
&lt;p>The following figure shows an example of the newly introduced magic-methods:&lt;/p>
&lt;p>&lt;img src="../profile.png" alt="example_profile">&lt;/p>
&lt;p>ðŸ“¢ From &lt;strong>FireDucks 0.12.2&lt;/strong>, &lt;code>fireducks.pandas&lt;/code> module internally loads &lt;code>fireducks.ipyext&lt;/code>. Therefore if &lt;code>fireducks.pandas&lt;/code> is already loaded as &lt;code>%load_ext fireducks.pandas&lt;/code>, then loading &lt;code>fireducks.ipyext&lt;/code> can be skipped.&lt;/p>
&lt;p>By default the profile shows the top-10 methods based on the execution time.
You may like to configure the same as follows: &lt;code>pd.options.styler.render.max_rows = &amp;lt;desired-number&amp;gt;&lt;/code>&lt;/p>
&lt;p>âš  Please do not use any import statement within the cell that is to be profiled using cell-magic: &lt;code>%%fireducks.profile&lt;/code>.
You may like to import the required libraries/modules in some different cell.&lt;/p>
&lt;h2 id="128073-measurement-for-expensive-fallback">ðŸ‘‰ Measurement for expensive fallback&lt;/h2>
&lt;p>If you find any method taking longer time due to fallback, you may either consider an alternative solution to avoid the fallback or
you may like to &lt;a href="https://github.com/fireducks-dev/fireducks/issues">report us&lt;/a> for supporting the method with a reproducible example.&lt;/p>
&lt;p>Also, when executing a python program, you may like to set the environment variable &lt;code>FIREDUCKS_FLAGS=&amp;quot;--trace=3&amp;quot;&lt;/code>.
This will produce a file named, &lt;code>trace.json&lt;/code> containing several traces related to your program performance in the current working directory.
If you like to share that json file with us (it doesn&amp;rsquo;t contain any confidential information related to your data),
we will look into the areas for improvement and may suggest you corrective measures.&lt;/p></description></item><item><title>Docs: Benchmark Archive</title><link>https://fireducks-dev.github.io/docs/benchmarks/archive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://fireducks-dev.github.io/docs/benchmarks/archive/</guid><description>
&lt;h2 id="2024-12-06">2024-12-06&lt;/h2>
&lt;p>Server specification (AWS EC2 m7i.8xlarge):&lt;/p>
&lt;ul>
&lt;li>cpu: Intel(R) Xeon(R) Platinum 8488C (32cores)&lt;/li>
&lt;li>main memory: 128GB&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://github.com/fireducks-dev/polars-tpch/tree/fireducks_20241206">source code of the benchmark&lt;/a>&lt;/p>
&lt;p>The following graph compares four data frame libraries
(&lt;a href="https://pandas.pydata.org/">pandas&lt;/a>,
&lt;a href="https://github.com/modin-project/modin">modin&lt;/a>, &lt;a href="https://pola.rs/">polars&lt;/a>,
and fireducks) on 22 different queries included in the benchmark. The vertical
axis shows how many times faster compared to pandas on a logarithmic scale,
where anything greater than 1 indicates that it is faster than pandas. &lt;strong>The
Scale Factor, which represents the data size is 10 (dataset of about 10 GB),
the time spent on each query was measured excluding I/O (RUN_IO_TYPE=skip),
and the input data is generated using pyarrow (instead of polars) for a fair
comparison.&lt;/strong> Please refer to the procedure mentioned in
&lt;a href="https://github.com/fireducks-dev/polars-tpch/blob/fireducks/README.md">README&lt;/a>
to reproduce the evaluation result.&lt;/p>
&lt;p>The average speedup over pandas for 22 queries was 1.0x for Modin, 57x for
Polars, and &lt;strong>125x for FireDucks&lt;/strong>.&lt;/p>
&lt;p>&lt;img src="../20241206/polars-tpch-sf10_20241205.webp" alt="polars-tpch-sf10">&lt;/p>
&lt;p>The versions of the libraries used were as follows (the latest versions at the time of the measurements).&lt;/p>
&lt;ul>
&lt;li>pandas: 2.2.3&lt;/li>
&lt;li>Modin: 0.32.0&lt;/li>
&lt;li>Polars: 1.6.0&lt;/li>
&lt;li>FireDucks: 1.1.2&lt;/li>
&lt;/ul>
&lt;h3 id="about-the-benchmark-code">About the benchmark code&lt;/h3>
&lt;p>This benchmark is originally from
&lt;a href="https://github.com/pola-rs/tpch">polars/tpch&lt;/a>. Because this repository
includes all 22 queries for polars but not all for pandas, we have implemented
all 22 queries using pandas, and then executed those using FireDucks by &lt;a href="https://fireducks-dev.github.io/docs/get-started/#import-hook">import hook&lt;/a>. These queries have been developed as
per the implementation in polars queries, so that the &lt;strong>apple-to-apple performance
comparison can be made on these two libraries&lt;/strong>. Since polars has different APIs
from pandas, there are minor differences in the implementation, but we kept the
number of time consuming operations like join/merge, filter, groupby etc. as
similar to that in polars implementation.&lt;/p>
&lt;h2 id="2024-09-09">2024-09-09&lt;/h2>
&lt;p>Server specification:&lt;/p>
&lt;ul>
&lt;li>cpu: intel(r) xeon(r) gold 5317 cpu @ 3.00ghz x 2sockets (48hw threads total)&lt;/li>
&lt;li>main memory: 256GB&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://github.com/fireducks-dev/polars-tpch/tree/fireducks">source code of the benchmark&lt;/a>&lt;/p>
&lt;p>The following graph compares four data frame libraries
(&lt;a href="https://pandas.pydata.org/">pandas&lt;/a>,
&lt;a href="https://github.com/modin-project/modin">modin&lt;/a>, &lt;a href="https://pola.rs/">polars&lt;/a>,
and fireducks) on 22 different queries included in the benchmark. The vertical
axis shows how many times faster compared to pandas on a logarithmic scale,
where anything greater than 1 indicates that it is faster than pandas. &lt;strong>The
Scale Factor, which represents the data size is 10 (dataset of about 10 GB),
the time spent on each query was measured excluding I/O (RUN_IO_TYPE=skip),
and the input data is generated using pyarrow (instead of polars) for a fair
comparison.&lt;/strong> Please refer to the procedure mentioned in
&lt;a href="https://github.com/fireducks-dev/polars-tpch/blob/fireducks/README.md">README&lt;/a>
to reproduce the evaluation result.&lt;/p>
&lt;p>The average speedup over pandas for 22 queries was 0.89x for Modin, 39x for
Polars, and &lt;strong>50x for FireDucks&lt;/strong>.&lt;/p>
&lt;p>&lt;img src="../20240909/polars-tpch-sf10_20240909.webp" alt="polars-tpch-sf10">&lt;/p>
&lt;p>The versions of the libraries used were as follows (the latest versions at the time of the measurements).&lt;/p>
&lt;ul>
&lt;li>pandas: 2.2.2&lt;/li>
&lt;li>Modin: 0.31.0&lt;/li>
&lt;li>Polars: 1.6.0&lt;/li>
&lt;li>FireDucks: 1.0.3&lt;/li>
&lt;/ul>
&lt;p>The following chart shows the comparison between Polars and FireDucks with
larger dataset, Scale Factor 10, 20 and 50. The vertical axis shows how many
times faster FireDucks is compared to Polars. On average, &lt;strong>FireDucks is 1.3
times (sf=10), 1.3 times (sf=20), and 1.5 times (sf=50) faster than Polars&lt;/strong>.&lt;/p>
&lt;p>&lt;img src="../20240909/polars-tpch_20240909.webp" alt="polars-tpch">&lt;/p>
&lt;h2 id="2024-06-05">2024-06-05&lt;/h2>
&lt;p>server specs&lt;/p>
&lt;ul>
&lt;li>cpu: intel(r) xeon(r) gold 5317 cpu @ 3.00ghz x 2sockets (48hw threads total)&lt;/li>
&lt;li>main memory: 256gb&lt;/li>
&lt;/ul>
&lt;h3 id="comparison-of-dataframe-libraries-using-tpc-h">comparison of dataframe libraries using tpc-h&lt;/h3>
&lt;p>&lt;a href="https://github.com/fireducks-dev/polars-tpch/tree/fireducks_20240605">source code of the benchmark&lt;/a>&lt;/p>
&lt;p>The following graph compares four data frame libraries
(&lt;a href="https://pandas.pydata.org/">pandas&lt;/a>,
&lt;a href="https://github.com/modin-project/modin">modin&lt;/a>, &lt;a href="https://pola.rs/">polars&lt;/a>,
and fireducks) on 22 different queries included in the benchmark. The vertical
axis shows how many times faster compared to pandas on a logarithmic scale,
where anything greater than 1 indicates that it is faster than pandas. The
Scale Factor, which represents the data size, is 10 (dataset of about 10 GB),
and the time spent on non-file IO was measured.&lt;/p>
&lt;p>The average speedup over pandas for 22 queries was 1.2x for Modin, 16x for
Polars, and &lt;strong>27x for FireDucks&lt;/strong>.&lt;/p>
&lt;p>&lt;img src="../20240605/polars-tpch-sf10_20240605.webp" alt="polars-tpch-sf10">&lt;/p>
&lt;p>The versions of the libraries used were as follows (the latest versions at the time of the measurements).&lt;/p>
&lt;ul>
&lt;li>pandas: 2.2.2&lt;/li>
&lt;li>Modin: 0.30.0&lt;/li>
&lt;li>Polars: 0.20.29&lt;/li>
&lt;li>FireDucks: 0.11.4&lt;/li>
&lt;/ul>
&lt;p>The following chart shows the comparison between Polars and FireDucks with
larger dataset, Scale Factor 10, 20 and 50. The vertical axis shows how many
times faster FireDucks is compared to Polars. On average, &lt;strong>FireDucks is 1.7
times (sf=10), 1.7 times (sf=20), and 1.8 times (sf=50) faster than Polars&lt;/strong>.&lt;/p>
&lt;p>&lt;img src="../20240605/polars-tpch_20240605.webp" alt="polars-tpch">&lt;/p>
&lt;h3 id="about-the-benchmark-code-1">About the benchmark code&lt;/h3>
&lt;p>This benchmark is originally from
&lt;a href="https://github.com/pola-rs/tpch">polars/tpch&lt;/a>. Because this repository
includes all 22 queries for polars but not all for pandas and Modin, we have
implemented all 22 queries using pandas then run those with FireDucks by
&lt;a href="https://fireducks-dev.github.io/docs/get-started/#import-hook">import hook&lt;/a>. Those queries were
also used with pandas and modin for the queries. All code for the queries is
available at
&lt;a href="https://github.com/fireducks-dev/polars-tpch/tree/fireducks_20240605">fireducks-dev/polars-tpch&lt;/a>&lt;/p>
&lt;p>&lt;strong>NOTE&lt;/strong>: Our pandas/modin/fireducks version of queries are implemented keeping two simple things in mind:&lt;/p>
&lt;ol>
&lt;li>the outcome of each query should match with the expected result.&lt;/li>
&lt;li>the code should look cleaner with the best possible usage of the chained methods, as there are several ways of implementing the same thing in pandas.&lt;/li>
&lt;/ol>
&lt;p>The queries for polars seem to be implemented with the different rule as described
&lt;a href="https://pola.rs/posts/benchmarks/#adjusting-for-dataframe-front-ends">here&lt;/a>.
Thus for the performance comparison with polars, someone might say the evaluation is not apple-to-apple
(as in DuckDB, polars which have more similar APIs to that of SQL). As far as the result is same and written in
most understandable format (with the best usage of the APIs), the benchmark can be accepted.&lt;/p>
&lt;h2 id="2024-02-06">2024-02-06&lt;/h2>
&lt;p>Server Specs&lt;/p>
&lt;ul>
&lt;li>CPU: Intel(R) Xeon(R) Gold 5317 CPU @ 3.00GHz x 2sockets (48HW threads total)&lt;/li>
&lt;li>Main memory: 256GB&lt;/li>
&lt;/ul>
&lt;h3 id="comparison-of-dataframe-libraries-using-tpc-h-1">Comparison of DataFrame libraries using TPC-H&lt;/h3>
&lt;p>&lt;a href="https://github.com/fireducks-dev/polars-tpch/tree/fireducks">Source code of the benchmark&lt;/a>&lt;/p>
&lt;p>The following graph compares four data frame libraries (&lt;a href="https://pandas.pydata.org/">pandas&lt;/a>, &lt;a href="https://github.com/modin-project/modin">modin&lt;/a>, &lt;a href="https://pola.rs/">polars&lt;/a>, and fireducks) on 22 different queries included in the benchmark. The vertical axis shows how many times faster compared to pandas on a logarithmic scale, where anything greater than 1 indicates that it is faster than pandas. The Scale Factor, which represents the data size, is 10 (dataset of about 10 GB), and the time spent on non-file IO was measured.&lt;/p>
&lt;p>The average speedup over pandas for 22 queries was 1.3x for Modin, 13x for Polars, and 18x for FireDucks.&lt;/p>
&lt;p>&lt;img src="../20240206/polars-tpch-sf10.webp" alt="polars-tpch-sf10">&lt;/p>
&lt;p>The versions of the libraries used were as follows (the latest versions at the time of the measurements).&lt;/p>
&lt;ul>
&lt;li>pandas: 2.2.0&lt;/li>
&lt;li>Modin: 0.26.1&lt;/li>
&lt;li>Polars: 0.20.7&lt;/li>
&lt;li>FireDucks: 0.9.8&lt;/li>
&lt;/ul>
&lt;p>The following chart shows the comparison results between Polars and FireDucks with larger dataset, Scalar Factor 10, 20 and 50. The vertical axis shows how many times faster FireDucks is compared to Polars. On average, FireDucks is 1.3 times (sf=10), 1.3 times (sf=20), and 1.7 times (sf=50) faster than Polars.&lt;/p>
&lt;p>&lt;img src="../20240206/polars-tpch_20240206.webp" alt="polars-tpch">&lt;/p>
&lt;h3 id="about-the-benchmark-code-2">About the benchmark code&lt;/h3>
&lt;p>This benchmark is originally from &lt;a href="https://github.com/pola-rs/tpch">polars/tpch&lt;/a>. Because this repository includes all 22 queries for polars but not all for pandas, we have implemented all 22 queries using pandas then run those with FireDucks by &lt;a href="https://fireducks-dev.github.io/docs/get-started/#import-hook">import hook&lt;/a>. Those queries were also used with pandas and modin for the queries polars/tpch does not provide. All code for the queries is available at &lt;a href="https://github.com/fireducks-dev/polars-tpch/commit/c5b148731bf3ee79f07d898c0d70d2ac8276ef7b">fireducks-dev/polars-tpch&lt;/a>.&lt;/p>
&lt;h2 id="2024-01-12">2024-01-12&lt;/h2>
&lt;p>Server Specs&lt;/p>
&lt;ul>
&lt;li>CPU: Intel(R) Xeon(R) Gold 5317 CPU @ 3.00GHz x 2sockets (48HW threads total)&lt;/li>
&lt;li>Main memory: 256GB&lt;/li>
&lt;/ul>
&lt;h3 id="comparison-of-dataframe-libraries-using-tpc-h-2">Comparison of DataFrame libraries using TPC-H&lt;/h3>
&lt;p>The following graph compares four data frame libraries (&lt;a href="https://pandas.pydata.org/">pandas&lt;/a>, &lt;a href="https://github.com/modin-project/modin">modin&lt;/a>, &lt;a href="https://pola.rs/">polars&lt;/a>, and fireducks) on 22 different queries included in the benchmark. The vertical axis shows how many times faster compared to pandas on a logarithmic scale, where anything greater than 1 indicates that it is faster than pandas. The Scale Factor, which represents the data size, is 10 (data set of about 10 GB), and the time spent on non-file IO is measured.&lt;/p>
&lt;p>The average speedup over pandas for 22 queries was 1.8x for Modin, 12x for Polars, and 17x for FireDucks.&lt;/p>
&lt;p>&lt;img src="../20240112/polars-tpch1.webp" alt="polars-tpch1">&lt;/p>
&lt;p>The versions of the libraries used were as follows (the latest versions at the time of the measurements).&lt;/p>
&lt;ul>
&lt;li>pandas: 2.1.4&lt;/li>
&lt;li>Modin: 0.26.0&lt;/li>
&lt;li>Polars: 0.20.2&lt;/li>
&lt;li>FireDucks: 0.9.3&lt;/li>
&lt;/ul>
&lt;p>For each query, the FireDucks development team implemented the program using pandas for pandas, modin, and FireDucks, and used it for modin and FireDucks with changing import statement. &lt;a href="https://github.com/pola-rs/tpch">polars/tpch&lt;/a> was used for Polars.&lt;/p>
&lt;p>The following chart shows the comparison results between Polars and FireDucks with larger dataset, Scalar Factor 10, 20 and 50. The vertical axis shows how many times faster FireDucks is compared to Polars. On average, FireDucks is 1.4 times (sf=10), 1.4 times (sf=20), and 1.6 times (sf=50) faster than Polars.&lt;/p>
&lt;p>&lt;img src="../20240112/polars-tpch2.webp" alt="polars-tpch2">&lt;/p></description></item><item><title>Docs: All Data Scientists</title><link>https://fireducks-dev.github.io/docs/use-case/data-scientist/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://fireducks-dev.github.io/docs/use-case/data-scientist/</guid><description>
&lt;p>As a data scientist, you want to minimize the complexity and latency of working with large data sets and increase your productivity. FireDucks can help you maximize your value as a data scientist.&lt;/p>
&lt;h2 id="time-is-money">Time is Money&lt;/h2>
&lt;p>Data preprocessing and analysis work can take up to 500 hours of waiting time each year. You can turn this time into time you can invest in other important tasks or new projects.&lt;/p>
&lt;h2 id="speedy-hypothesis-testing">Speedy hypothesis testing&lt;/h2>
&lt;p>Quick analysis cycles allow you to make decisions efficiently. The speed of business can be dramatically improved by smoothly rotating the cycle from hypothesis formulation to validation and re-examination.&lt;/p>
&lt;h2 id="rapid-machine-learning-models-implementation">Rapid machine learning models implementation&lt;/h2>
&lt;p>FireDucks is particularly effective in accelerating feature creation. They provide powerful support during the model building phase.&lt;/p>
&lt;p>&lt;img src="../../../images/G-1313496084-640x360.webp" alt="data scientist">&lt;/p></description></item><item><title>Docs: Highly suitable use cases</title><link>https://fireducks-dev.github.io/docs/use-case/industries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://fireducks-dev.github.io/docs/use-case/industries/</guid><description>
&lt;h2 id="automobile">Automobile Industry&lt;/h2>
&lt;p>&lt;img src="../../../images/G-971246808-640x360.webp" alt="automobile">&lt;/p>
&lt;p>In recent years, cars have been equipped with technology that records information such as our driving history and brake strength in real time. This information is called &amp;ldquo;probe data&amp;rdquo;, and FireDucks and these data can be used to&lt;/p>
&lt;p>&lt;strong>Weather Prediction&lt;/strong>: Build a prediction model using wiper and outside temperature data. This will revolutionize the data business in the automotive industry.&lt;/p>
&lt;p>&lt;strong>Marketing&lt;/strong>: Helps you find the best locations for new store openings based on nearby parking lot usage. And improve your customers&amp;rsquo; experience with faster marketing tools and real-time dashboard updates.&lt;/p>
&lt;p>&lt;strong>Traffic Improvements&lt;/strong>: Identify congested or hazardous sites and make recommendations to resolve them. Accidents can also be prevented, for example by adding signs to areas where emergency braking is common.&lt;/p>
&lt;p>One caveat, however, is that FireDucks is currently limited in its ability to process very large data sets simultaneously on multiple servers. However, once the data is organized, FireDucks can be used to its full potential. This supports efficient and rapid implementation of innovative initiatives such as those described above.&lt;/p>
&lt;h2 id="telecom">Telecommunications Industry&lt;/h2>
&lt;p>&lt;img src="../../../images/G-1198489792-640x360.webp" alt="telecom">&lt;/p>
&lt;p>FireDucks maximizes the potential of location-based businesses, which are expected to become even more valuable in the future.&lt;/p>
&lt;p>&lt;strong>Faster Location Information Processing&lt;/strong>: User location information is critical to the delivery of a wide range of services and features in the telecommunications industry. Faster processing of this information improves real-time performance and quality of service.&lt;/p>
&lt;p>&lt;strong>Rapid Service Response&lt;/strong>: Faster location-based services and information delivery reduces response time to end-users, resulting in improved user satisfaction and new services. This is expected to increase user satisfaction and expand new business opportunities.&lt;/p>
&lt;p>&lt;strong>Optimize edge computing&lt;/strong>: Optimize edge computing performance by processing location information in real time. Reduced communication latency enables smoother service delivery.&lt;/p>
&lt;p>&lt;strong>Develop new location-based services&lt;/strong>: Benefit from faster data processing and gain flexibility to develop new location-based services and features. This allows for quicker response to market needs.&lt;/p>
&lt;h2 id="finance">Financial Industry&lt;/h2>
&lt;p>&lt;img src="../../../images/G-1269677040-640x360.webp" alt="finance">&lt;/p>
&lt;p>FireDucks provides solutions to make financial data processing more efficient and secure, where reliability and speed are essential.&lt;/p>
&lt;p>&lt;strong>High-speed processing of complex financial data&lt;/strong>: Daily trading data, customer information, market trends, and other large volumes of data require fast batch aggregation, and FireDucks can dramatically increase the speed of processing.&lt;/p>
&lt;p>&lt;strong>Stable service&lt;/strong>: Greatly reduces the risk of timeout errors, even when data processing increases due to new user growth or market fluctuations. Provide more reliable service to your customers.&lt;/p>
&lt;p>&lt;strong>Efficiently link to subsequent operations&lt;/strong>: Faster processing speeds allow for quicker data linkage to subsequent operations and other systems. This significantly reduces the risk of operational delays and errors by ensuring that the entire batch process is completed within the specified time.&lt;/p>
&lt;p>&lt;strong>Support for real-time decision making&lt;/strong>: In the financial industry, there are many situations where quick decision making is required. The ability to process and analyze data at high speed allows for more effective real-time analysis of market trends and risk management.&lt;/p>
&lt;h2 id="cloudhosting">Cloud Hosting Industry&lt;/h2>
&lt;p>&lt;img src="../../../images/G-1312418673-640x360.webp" alt="cloudhosting">&lt;/p>
&lt;p>FireDucks can provide added value to cloud users who are concerned about latency and error risk in their daily batch aggregation processes.&lt;/p>
&lt;p>&lt;strong>Minimize Errors&lt;/strong>: Data processing times increase with the number of new users. However, our technology&amp;rsquo;s accelerated batch processing significantly reduces the timeout error rate. This allows us to continue to provide stable service.&lt;/p>
&lt;p>&lt;strong>Efficient task transitions&lt;/strong>: Tasks can be passed smoothly without making subsequent processes wait. These fast transitions shorten the overall batch processing completion time. The risk of a process not completing in time is greatly reduced.&lt;/p>
&lt;p>&lt;strong>Resource Optimization&lt;/strong>: Faster batch processing makes better use of required resources and reduces excessive costs. At the same time, processing efficiency reduces energy consumption, making it an economical and eco-friendly choice.&lt;/p>
&lt;h2 id="e-commerce">E-commerce Industry&lt;/h2>
&lt;p>&lt;img src="../../../images/G-1327127856-640x360.webp" alt="e-commerse">&lt;/p>
&lt;p>With FireDucks, you can improve the efficiency of your recommendation engine, which directly leads to increased user satisfaction and sales.&lt;/p>
&lt;p>&lt;strong>Data-driven optimization&lt;/strong>: The output from a vast amount of information, such as customer gender, age, browsing history, click history, purchase history, etc., is critical data for recommending the best products for each individual customer. However, processing this data is extremely time-consuming. This is where FireDucks technology comes in.&lt;/p>
&lt;p>&lt;strong>Faster, more effective recommendations&lt;/strong>: FireDucks dramatically reduces data preprocessing time. This allows you to explore more advanced and effective recommendation engine algorithms that will not only improve your customers&amp;rsquo; buying experience, but will also significantly increase your sales.&lt;/p>
&lt;p>&lt;strong>Timely Marketing Initiatives&lt;/strong>: Reduced processing times allow for timely promotions and campaigns. The reduced processing time enables the development of timely promotions and campaigns that match the seasonality and trends, attracting the interest of customers and leading to immediate purchases.&lt;/p>
&lt;h2 id="game">Smartphone Game App Industry&lt;/h2>
&lt;p>&lt;img src="../../../images/G-1324678076-640x360.webp" alt="game">&lt;/p>
&lt;p>FireDucks can be used to accelerate data analysis to prevent user abandonment and further increase sales.&lt;/p>
&lt;p>&lt;strong>User behavior analysis&lt;/strong>: At what stage of the game does a user leave, and what triggers the decision to pay? Finding the answers to these questions from data is the key to the success of a game. This information is hidden in a vast amount of data.&lt;/p>
&lt;p>&lt;strong>Measurement realization&lt;/strong>: FireDucks has dramatically accelerated the data analysis process. This allows data scientists to focus on analysis and to quickly consider and implement effective measures, such as the optimal timing for tutorials and when to offer free gachas. As a result, we can expect longer user dwell time and reduced abandonment rates.&lt;/p>
&lt;p>&lt;strong>Cloud cost reduction&lt;/strong>: The cost of data aggregation using AWS or GCP increases in proportion to the processing time. However, FireDucks&amp;rsquo; acceleration technology reduces this processing time, resulting in significant savings on cloud services.&lt;/p>
&lt;p>&lt;strong>Optimize the player experience&lt;/strong>: Rapid analytics allow you to optimize in-game activities and promotions in real time according to user behavior. This is expected to increase user satisfaction and encourage continuous billing.&lt;/p></description></item><item><title>Docs: FAQ</title><link>https://fireducks-dev.github.io/docs/help/faq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://fireducks-dev.github.io/docs/help/faq/</guid><description>
&lt;ul>
&lt;li>What is the data format FireDucks internally uses.
&lt;ul>
&lt;li>FireDucks &lt;a href="https://fireducks-dev.github.io/docs/user-guide/03-acceleration/#multithreading">multithreaded CPU backend&lt;/a> uses &lt;a href="https://arrow.apache.org/">Apache Arrow&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>What is the lincense of FireDucks:
&lt;ul>
&lt;li>The 3-Clause BSD License&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>About supported platforms
&lt;ul>
&lt;li>FireDucks is currently available for Linux (&lt;code>manylinux&lt;/code>) on the x86_64 architecture.&lt;/li>
&lt;li>We will support Windows etc. if many requests.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>About support
&lt;ul>
&lt;li>Is there support for FireDucks?
&lt;ul>
&lt;li>As far as we know, no organization offers paid support for FireDucks.&lt;/li>
&lt;li>We have a &lt;a href="https://join.slack.com/t/fireducks/shared_invite/zt-2j4lucmtj-IGR7AWlXO62Lu605pnBJ2w">Slack workspace&lt;/a> where the FireDucks development team and users discuss.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>I found a problem, who should I contact?
&lt;ul>
&lt;li>Please register &lt;a href="https://github.com/fireducks-dev/fireducks/issues">an issue on the GitHub repository&lt;/a>.&lt;/li>
&lt;li>Or please email &lt;a href="mailto:contact@fireducks.jp.nec.com">mailto:contact@fireducks.jp.nec.com&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Can I send a request?
&lt;ul>
&lt;li>Of course! We may not be able to respond to all requests, but all requests are welcome. Please see &lt;a href="https://fireducks-dev.github.io/docs/help/contact/">contact information&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Where can I get release information for FireDucks?
&lt;ul>
&lt;li>Please use &lt;a href="https://pypi.org/help/#project-release-notifications">release notifications on pypi.org&lt;/a>.&lt;/li>
&lt;li>We will also post announcements on &lt;a href="https://twitter.com/fireducksdev">X (Twitter)&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: Contact</title><link>https://fireducks-dev.github.io/docs/help/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://fireducks-dev.github.io/docs/help/contact/</guid><description>
&lt;ul>
&lt;li>&lt;a href="https://github.com/fireducks-dev/fireducks/issues">Issues on GitHub&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://twitter.com/fireducksdev">X (Twitter)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://join.slack.com/t/fireducks/shared_invite/zt-2j4lucmtj-IGR7AWlXO62Lu605pnBJ2w">Slack&lt;/a>&lt;/li>
&lt;li>&lt;a href="mailto:contact@fireducks.jp.nec.com">E-mail&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>