<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on</title><link>https://fireducks-dev.github.io/ja/posts/</link><description>Recent content in Posts on</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Tue, 05 Dec 2023 00:00:00 +0900</lastBuildDate><atom:link href="https://fireducks-dev.github.io/ja/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>FireDucks内部で働く高速化技術</title><link>https://fireducks-dev.github.io/ja/posts/est/</link><pubDate>Tue, 05 Dec 2023 00:00:00 +0900</pubDate><guid>https://fireducks-dev.github.io/ja/posts/est/</guid><description>groupbyの切り替え #
この記事ではFireDucks内部で使われているgroupby高速化技術について紹介します．
表データ分析においてgroupby操作は最も基本的で重要な操作の一つです． groupby操作を用いることでデータの平均や分散といった重要な統計的性質を知ることができ， その他操作と組み合わせることで新しい特徴量を得ることもできます．
FireDucksでは高速なgroupby操作を実現するためにデータ特性に応じた最適化を行っています． その一つがグループ数によるgroupbyアルゴリズムの自動選択です． 具体的にはデータのグループ数に注目しグループ数が小さいデータに対して高速に計算可能な アルゴリズム(アルゴリズムAと呼ぶ)と， グループ数が大きいデータに対して高速に計算可能なアルゴリズム(アルゴリズムBと呼ぶ)とを切り替えています． ここでグループ数とは注目するカラムを構成するデータの種類を示します．
例えば以下のような表データを考えてみましょう．
food category 0 apple fruit 1 carrot vegetable 2 peach fruit 3 onion vegetable 4 grape fruit foodカラムを構成する要素に注目するとapple, carrot, peach, onion, grapeの5種類であることが分かります． よってfoodカラムのグループ数は5です． 一方で，categoryカラムを構成する要素に注目するとfruit,vegetableの2種類であることが分かります． よってcategoryカラムのグループ数は2です．
グループ数の推定 #
巨大なデータを扱うときに実際にデータを全て確認してグループ数を計算すると時間がかかります． そこで，FireDucksではグループ数の厳密な値を求めることなく統計的手法を用いてグループ数の推定を行っています1． 具体的には以下の手順で行われます．
注目するグループキーのデータ列の中からランダムにデータを一つ取り出す． 取り出したデータを記録する． 再び注目するグループキーのデータ列の中からランダムにデータを一つ取り出す， すでに記録してあるデータと一致するか判定を行い，データを記録する． 3,4の操作を一定回数繰り返す． 3と4の操作を一定回数繰り返した後，データを取り出した回数と一致した回数から 注目するグループキーのデータのグループ数を推定する．
性能評価 #
データ分析に関する多くの処理を含むベンチマークであるTPC-Hを用いて計算速度の計測を行いました．
Aはgroupby操作をアルゴリズムAのみを用いており，Bはgroupby操作をアルゴリズムBのみを用いています． autoはgroupby操作についてグループ数の推定とアルゴリズムAとBの自動選択をしています． アルゴリズムAとアルゴリズムBの実行時間にほとんど差がない処理については上のグラフでは省略しています． グラフから，q10を除く処理で自動判定アルゴリズムは，アルゴリズムAとアルゴリズムBのうち速い方のアルゴリズムの選択ができていることが分かります． totalはアルゴリズムA,アルゴリズムB,自動選択アルゴリズムについて上記グラフから除いた処理も含めたTPC-H全体の計算時間を示しています． このことからTPC-H全体において，自動判定アルゴリズムはアルゴリズムAに対して3倍程度高速化ができており， アルゴリズムBに対しては1.2倍程度高速化ができていることが分かります．
参考 #
M. Bressan, E. Peserico, and L. Pretto. Simple set cardinality estimation through random sampling.</description></item><item><title>インポートフック：ソースコードを書き換えずに FireDucks を使う方法のご紹介</title><link>https://fireducks-dev.github.io/ja/posts/importhook/</link><pubDate>Wed, 15 Nov 2023 09:35:10 +0900</pubDate><guid>https://fireducks-dev.github.io/ja/posts/importhook/</guid><description>FireDucks 開発チームの大道です．今日は開発者ブログとして，FireDucks に備わっているインポートフック機能をご紹介したいと思います．この機能を使えば，お手持ちのソースコードを1行も書き換えることなく FireDucks を使うことができるようになります．
コマンドラインから Python ファイルを実行する際の使い方と，IPython や Jupyter Notebook での使い方を見ていきましょう．
インポートフックとは？ #
FireDucks はオリジナルの pandas と同じように振る舞うため，インポート文を以下のように書き換えるだけで簡単に使い始めることができます．
# import pandas as pd import fireducks.pandas as pd しかしたかが1行書き換えるだけとはいえ，今まで作成してきたプログラムで pandas を使用している部分を探して全部 FireDucks に置き換えるのは，意外と面倒くさいものです．また，pandas と連携するサードパーティライブラリでも FireDucks を使用したい場合，そのライブラリの中にまで手を入れて import pandas を書き換える作業は，普通はしたくないでしょう．
Get Started でも触れられている通り，FireDucks にはインポートフックというユーティリティが含まれています．コマンドラインで your_script.py を実行する際に，Python インタープリターに以下のようにオプションを指定してください．
python3 -m fireducks.imhook your_script.py 以上のようにして起動すると，pandas をインポートしようとしたら代わりに fireducks.pandas がインポートされるようになります．これは your_script.py のソースコードを編集しているわけではなく，プログラムを実行しながら動的にインポート処理に割り込んでいます．
インポートフックの動作例 #
以下のような簡単な Python スクリプト print_classname.py で動作を確認してみましょう．このスクリプトでは DataFrame クラスの repr 表現が出力されます．
import pandas as pd print(pd.</description></item><item><title>Pythonの高速データフレームライブラリFireDucksを使ってみた</title><link>https://fireducks-dev.github.io/ja/posts/nes_taxi/</link><pubDate>Mon, 23 Oct 2023 08:47:36 +0000</pubDate><guid>https://fireducks-dev.github.io/ja/posts/nes_taxi/</guid><description>pandasは，プログラミング言語Pythonにおいて，データ解析を支援する機能を提供するライブラリである． NECの研究所では高速化版pandasであるFireDucksというライブラリを開発している．
データの準備 #
ニューヨークのタクシーの乗降者履歴のデータを対象に分析を行う． データの出典は以下である：
https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page
大規模データの解析を行うため，上記リンクから「Yellow Taxi Trip Records」を2022年1月から2023年6月までのデータをダウンロードし結合した． データはparquet形式で提供されているが，日頃良く使うcsv形式で試すために変換しておく． 参考までにデータ準備用のスクリプトを付記しておく．
import pandas as pd import os dir = &amp;quot;xxx&amp;quot; df_list = [] for year in [2022, 2023]: for i in range(12): month = str(i+1).zfill(2) fn = f&amp;quot;yellow_tripdata_{year}-{month}.parquet&amp;quot; file = os.path.join(dir, fn) if not os.path.exists(file): continue df = pd.read_parquet(fn) df_list.append(df) all_df = pd.concat(df_list) all_df.to_csv(&amp;quot;taxi_all.csv&amp;quot;) データの中身は以下のような値が入っている（列は一部抜粋）．
列名 データ型 説明 passenger_count int 乗車人数 pu_location_Id string タクシーメーターが作動し始めたTLCタクシーゾーン． do_location_Id string タクシーメーターが解除されたTLCタクシーゾーン． tpep_dropoff_datetime string メーターが解除された日時． tpep_pickupdate_time string メーターが作動し始めた日時． trip_distance double タクシーメーターによって報告された走行距離（マイル単位）． total_amount double 乗客に請求される合計金額． ※現金のチップは含まれない． extra double その他の割増料金と追加料金．現在，これには0.</description></item></channel></rss>